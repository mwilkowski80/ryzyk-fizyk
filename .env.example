# Opcjonalnie: ścieżka do pliku .env (gdy chcesz trzymać konfigurację gdzie indziej)
# Uwaga: ta zmienna jest czytana PRZED załadowaniem .env, więc ustaw ją w środowisku systemowym.
# ENV_FILE=.env

# Bazowy URL do serwera LLM (OpenAI-compatible). Przykłady:
# - lokalny: http://localhost:8000
# - z proxy:  https://api.twoj-provider.com
LLM_BASE_URL=http://localhost:8000

# Nazwa modelu po stronie serwera LLM (np. "gpt-4o-mini", "llama-3.1-70b", itp.)
LLM_MODEL=gpt-4o-mini

# Klucz API (jeśli serwer go wymaga). Zostaw puste tylko jeśli Twój serwer nie wymaga autoryzacji.
LLM_API_KEY=

# Temperatura generacji (0.0 = bardziej deterministycznie; 1.0+ = bardziej kreatywnie)
LLM_TEMPERATURE=0.7

# Maksymalna liczba tokenów odpowiedzi (mniejsze = szybciej na słabszych modelach)
LLM_MAX_TOKENS=256

# Wymuszenie formatu odpowiedzi (jeśli serwer wspiera OpenAI-compatible response_format)
# - none: domyślnie wyłączone dla kompatybilności
# - json_object: może poprawić poprawność JSON, ale nie każdy serwer to wspiera
LLM_RESPONSE_FORMAT=none

# Timeout HTTP w sekundach
LLM_TIMEOUT_SECONDS=30

# Ile razy aplikacja ma ponowić generowanie, jeśli LLM zwróci niepoprawny JSON
LLM_MAX_RETRIES=5

# Ścieżka endpointu chat completions (jeśli Twój serwer używa innej)
LLM_CHAT_COMPLETIONS_PATH=/v1/chat/completions

# --- Prefetch/cache kart (przyspiesza 'next' w CLI i Web) ---

# Ile kart trzymać w pamięci (docelowo)
POOL_TARGET_SIZE=25

# Gdy liczba kart spadnie poniżej tego progu, pula zacznie się uzupełniać w tle
POOL_REFILL_THRESHOLD=10

# Ile kart próbować pozyskać w pojedynczym zapytaniu do LLM
# (1 = najszybsze pierwsze pytanie na wolnych modelach; większe może być bardziej wydajne, ale ryzykuje timeout)
POOL_BATCH_SIZE=1

# Ile równoległych zapytań do LLM robić podczas uzupełniania (zwiększaj ostrożnie)
POOL_CONCURRENCY=1

# --- Opcje dla wersji webowej (python -m number_questions.web / number-questions-web) ---

# Interfejs na którym ma nasłuchiwać serwer web (domyślnie 127.0.0.1)
WEB_HOST=127.0.0.1

# Port serwera web (domyślnie 8001)
WEB_PORT=8001

# Flaga tylko do testów automatycznych: pozwala na POST /shutdown
# (domyślnie 0; ustaw 1 tylko lokalnie, gdy chcesz zautomatyzować testy UI)
WEB_ALLOW_SHUTDOWN=0
